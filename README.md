# data-science-IBM

# Data Science by IBM

- [Data Science by IBM](#data-science-by-ibm)
  - [What is Data Science -- Week 1](#what-is-data-science----week-1)
  - [What is Data Science -- Week 2](#what-is-data-science----week-2)
  - [What is Data Science -- Week 3](#what-is-data-science----week-3)
  - [Open Source tools for Data Science -- Week 1](#open-source-tools-for-data-science----week-1)
  - [Open Source tools for Data Science -- Week 2](#open-source-tools-for-data-science----week-2)
  - [Open Source tools for Data Science -- Week 3](#open-source-tools-for-data-science----week-3)
  - [Data Science Methodology -- Week 1](#data-science-methodology----week-1)
    - [From Problem to Approach](#from-problem-to-approach)
    - [From Requirements to Collection](#from-requirements-to-collection)
  - [Data Science Methodology -- Week 2](#data-science-methodology----week-2)
    - [From Understanding to Preparation](#from-understanding-to-preparation)
    - [From Modelling to Evaluation](#from-modelling-to-evaluation)
  - [Data Science Methodology -- Week 3](#data-science-methodology----week-3)
    - [From Deployment to Feedback](#from-deployment-to-feedback)
  - [Python for Data Science and AI -- Week 1](#python-for-data-science-and-ai----week-1)
  - [Python for Data Science and AI -- Week 2](#python-for-data-science-and-ai----week-2)
  - [Python for Data Science and AI -- Week 3](#python-for-data-science-and-ai----week-3)
  - [Python for Data Science and AI -- Week 4](#python-for-data-science-and-ai----week-4)
  - [Python for Data Science and AI -- Week 5](#python-for-data-science-and-ai----week-5)
  - [Databases and SQL for Data Science -- Week 1](#databases-and-sql-for-data-science----week-1)
  - [Databases and SQL for Data Science -- Week 2](#databases-and-sql-for-data-science----week-2)
  - [Databases and SQL for Data Science -- Week 3](#databases-and-sql-for-data-science----week-3)
  - [Databases and SQL for Data Science -- Week 4](#databases-and-sql-for-data-science----week-4)
  - [Data Analysis with Python -- Week 1](#data-analysis-with-python----week-1)
  - [Data Analysis with Python -- Week 2](#data-analysis-with-python----week-2)
  - [Data Analysis with Python -- Week 3](#data-analysis-with-python----week-3)
  - [Data Analysis with Python -- Week 4](#data-analysis-with-python----week-4)
  - [Data Analysis with Python -- Week 5](#data-analysis-with-python----week-5)
  - [Data Visualization with Python -- Week 1](#data-visualization-with-python----week-1)
  - [Data Visualization with Python -- Week 2](#data-visualization-with-python----week-2)
  - [Data Visualization with Python -- Week 3](#data-visualization-with-python----week-3)
  - [Machine Learning with Python -- Week 2](#machine-learning-with-python----week-2)
    - [Linear Regression](#linear-regression)
    - [Multiple Linear Regression](#multiple-linear-regression)
    - [Polynomial Regression](#polynomial-regression)
  - [Machine Learning with Python -- Week 3](#machine-learning-with-python----week-3)
    - [K-Nearest Neighbours](#k-nearest-neighbours)
    - [Decision Trees](#decision-trees)
    - [Logistic Regression](#logistic-regression)
    - [Support Vector Machine (SVM)](#support-vector-machine-svm)
  - [Machine Learning with Python -- Week 4](#machine-learning-with-python----week-4)
    - [K-Means](#k-means)
    - [Hierarchical Clustering](#hierarchical-clustering)
    - [Density-based Clustering](#density-based-clustering)
  - [Machine Learning with Python -- Week 5](#machine-learning-with-python----week-5)
    - [Content-based Recommender](#content-based-recommender)
    - [Collaborative Filtering](#collaborative-filtering)
  - [Applied Data Science Capstone -- Week 1](#applied-data-science-capstone----week-1)
  - [Applied Data Science Capstone -- Week 2](#applied-data-science-capstone----week-2)
  - [Applied Data Science Capstone -- Week 3](#applied-data-science-capstone----week-3)


## What is Data Science -- Week 1

Data Science is the field of exploring, manipulating, and analyzing data, and using data to answer questions or make recommendations.

As Data Science is not a discipline traditionally taught at universities, contemporary data scientists come from diverse backgrounds such as engineering, statistics, and physics.

*Quiz* 010101

1. Harvard Business Review called Data Science the sexiest job in the 21st century. -- **True**
2. According to the report by the McKinsey Global Institute, by 2018, it is projected that there will be a shortage of 140,000 - 190,000 people with deep analytical skills in the world. -- **False (in US alone)**
3. Walmart addressed its analytical needs by approaching Kaggle to host a competition for analyzing its proprietary data. -- **True**
4. What is the average base salary for a data scientist reported by the New York Times? -- **$112,000**
5. According to professor Haider, the three important qualities to possess in order to succeed as a data scientist are curious, judgemental, and proficient in programming. -- **False (argumentative as the last one)**

Using complicated machine learning algorithms does not always guarantee achieving a better performance. Occasionally, a simple algorithm such as k-nearest neighbor can yield a satisfactory performance comparable to the one achieved using a complicated algorithm. It all depends on the data.

The cloud allows you to bypass the physical limitations of your personal computer and the systems you are using. One limitation of the cloud is that you are not able to deploy capabilities of advanced machines that do not necessarily have to be your machines.

*Quiz* 010102

1. Hal Varian, the chief economist at Google, declared that "the sexy job in the next ten years will be **statisticians**, but it is widely believed that what he really meant were **data scientists**.
2. The author defines a data scientist as someone who finds solutions to problems by analyzing data using appropriate tool and then tells stories to communicate their finding to the relevant stakeholders. -- **True**
3. According to the reading, the author defines data science as the art of uncovering the hidden secrets in data. -- **False**
4. What is admirable about Dr. Patil's definition of a data scientist is that it limits data science to activities involving machine learning. -- **False (that it is inclusive of individuals of various academic backgrounds and training and does not restrict the definition of a data scientist to a particular tool or subject it to a certain arbitrary minimum threshold of data size)**
5. According to the reading, the characteristics exhibited by the best data scientists are those who are curious, ask good questions, and have at least 10 years of experience. -- **False**


## What is Data Science -- Week 2

Data science and business analytics became very hot subjects recently, since around the year 2012.

Big data was started by Google when Google tried to figure out how to solve their PageRank algorithm.

Big data is data that is large enough and has enough volume and velocity that you cannot handle it with traditional database systems.

According to Dr. White, most of the components of data science, such as probability, statistics, linear algebra, and programming, have been around for many decades but now we have the computational capabilities to apply combine them and come up with new techniques and learning algorithms.

*Quiz* 010201

1. According to the reading, the output of a data mining exercise largely depends on the skills of the data scientist carrying out the exercise. -- **False**
2. When data are missing in a systematic way, you can simply extrapolate the data or impute the missing data by filling in the average of the values around the missing data. -- **False (when data are missing in a systematic way, you should determine the impact of missing data on the results and whether missing data can be excluded from the analysis)**
3. Prior Variable Analysis and Principal Component Analysis are both examples of a data reduction algorithm. -- **False (Prior Variable Analysis is not a data reduction algorithm)**
4. After the data are appropriately processed, transformed, and stored, machine learning and non-parametric methods are a good starting point for data mining. -- **False**
5. In-sample forecast is the process of formally evaluating the predictive capabilities of the models developed using observed data to see how effective the algorithms are in reproducing data. -- **True** 

Neural networks have been around for decades, but people decided not to develop them any more because they were computationally very expensive.

The use cases for deep learning include speech recognition and classifying image at a large scale.

According to Dr. White, if someone is coming into a data science team, the first skills they would need are: knowing basic probability and some basic statistics, knowing some algebra and some calculus, understanding relational databases, knowing how to program, at least have some computational thinking.

According to Dr. White, the industrial world is shifting to a new trend, and for high school students to be on the right side of this new trend, his advice to them is: 
* take a course in probability
* learn how to program
* learn some math
* try to start experimenting with building small systems that work and are useful
* learn statistics

Netflix uses machine learning to recommend movies to you based on movies you have already watched and liked or disliked.

*Quiz* 010202

1. The real added value of the author's research on residential real estate is quantifying the magnitude of relationships between housing prices and different determinants. -- **True**
2. Regression is a statistical technique developed by Sir Frances Galton. -- **True**
3. According to the reading, the author discovered that an additional bedroom adds more to the housing prices than an additional washroom. -- **False (other way round)**
4. The author discovered that, all else being equal, houses located less than 5km but more than 2km to shopping centres sold for more than the rest. -- **True**
5. "What are typical land taxes in a house sale?" is a question that can be put to regression analysis. -- **False**


## What is Data Science -- Week 3

What are some of the first steps that companies need to take to get started in data science? 
* Start collecting data
* Put together a team of data scientists

Curiosity is one of the most important skills that a data scientist should have in addition to sense of humor and story telling.

When hiring a data scientist, you need to ensure that the candidate is passionate about your field of work. A brilliant data scientist who is passionate about the field of IT won't necessary excel in the field of healthcare if they are passionate about it.

Which of the following are applications of data science?
* Augmented Reality
* Optimizing resource allocation for relief aid
* Determining if an application for a credit card should be accepted based on the applicant's financial history and data

*Quiz* 010301

1. According to the reading, what is the ultimate purpose of analytics? -- **To communicate findings to stakeholders to formulate policy or strategy**
2. The reading mentions a common role of a data scientist is to use analytics insights to build a narrative to communicate findings to stakeholders. -- **True**
3. The United States Economic Forecast is a publication by -- **Deloitte University Press**
4. The report discussed in the reading successfully did the job of -- **using data and analytics to generate likely economic scenarios**
5. According to the reading, it is recommended that a team waits until the results of analytics are out before they can decide on the final deliverable. -- **False (in order to produce a compelling narrative, initial planning and conceptualizing of the final deliverable is of extreme importance)**

*Quiz* 010302

1. Regardless of the length of the final deliverable, the author recommends that it includes a cover page, table of contents, executive summary, detailed contents, acknowledgements and references. -- **True**
2. An introductory section is always helpful in -- **setting up the problem for the read who might be new to the topic**
3. The result section is where you present -- **your empirical findings**
4. The discussion section is where you -- **craft your main arguments by building on the results; rely on the power of narrative to enable numbers to communicate your thesis to your readers; highlight how your findings provide the ultimate missing piece to the puzzle**
5. Adding a list of references and an acknowledgement section are examples of housekeeping, according to the author. -- **True**


## Open Source tools for Data Science -- Week 1

Skill Network Labs is an environment that contains tools such as RStudio, Jupyter Network, and Zeppelin Notebook. These tools provide an interactive environment for you to perform data analysis, data visualization, machine learning and image recognition. 

*Quiz* 

1. What is Skills Network Labs? -- **an environment that saves time: from installing, configuring and maintaining tools; a web-based environment that data science practitioners and enthusiasts alike can use for free; an online virtual lab environment that includes several open data science tools in the clouds**
2. Skills Network Labs provides an integrated environment to -- **learn and try various, popular data science tools; use tools such as Python and Scala; use tools such as R and Spark; save your time without needing to install and configure these tools**
3. Zeppelin and Jupyter Notebooks are great for -- **interactive data analytics**
4. Which of the following feature of RStudio specifically enables you to create interactive apps or visualizations? -- **script editor**
5. Basic charts are already included in Apache Zeppelin, allowing you to convert data tables directly into visualizations without any code. -- **True**

Jupyter Notebooks are "interactive" documents that enable you to write and execute code in chunks (as opposed to all-or-none), add explanatory text, write mathematical equations. They are popular in data science as you can do most of the data science work (importing data, cleaning data, analyzing data, visualizing data, machine learning) and you can also share the notebooks to collaborate with others.

Jupyter Notebook refers the actual file while JupyterLab is the environment that organizes my various Jupyter Notebooks that allows me to run them.

To write and execute code in Jupyter Notebooks, first you need to write code in one of the gray code cells, and then press the Play button to execute the code. Alternatives to pressing the Play button include keyboard shortcuts such as Shift+Enter and Ctrl+Enter to run the code.

*Quiz* 

1. What can you write in Jupyter Notebooks? -- **code to be executed in one of the kernels (e.g. Python / R / Scala); stylized text in a format called "Markdown"; HTML code that can be written and rendered via Markdown cells**
2. What were Jupyter Notebooks called before the name was changed to Jupyter? -- **IPython notebooks**
3. If you wrote "# Hi there" in Markdown, this would be the equivalent of writing \<h1>Hi there\<\h1> in HTML. -- **True**
4. Which of the following options are TRUE? -- **download and save Jupyter Notebooks as .ipynb files; change kernels within a Jupyter Notebook; connect to databases from within Jupyter Notebooks**
5. Although you can change the kernel of the Jupyter Notebook between different programming languages (e.g. Python / R / Scala), you cannot use multiple kernels within the same Jupyter notebook (e.g. running Python, R and Scala within the same notebook). -- **True**


## Open Source tools for Data Science -- Week 2

Zeppelin notebooks come with the following built in: Apache Spark, certain data visualizations and pivot charts.

You can use multiple programming languages in the same Zeppelin notebook. Note that Scala language and SQL language are used in different cells, but were operating one the same data. This is the key difference between Jupyter and Zeppelin notebooks; in Jupyter notebooks, you can only use one language at a time.

What is the filetype for Zeppelin notebooks? .json

*Quiz*

1. Apache Zeppelin lets you -- **document your code; display your output; show your visualizations within the Zeppelin netbook; switch between multiple languages**
2. Which is NOT an interpreter supported by Zeppelin? -- **Matlab**
3. Major benefits to using Zeppelin over Jupyter notebooks include -- **some visualizations are automatically generated in Zeppelin, thus minimizing code required; you can use multiple languages in the same notebook in Zeppelin, but not in Jupyter**
4. You cannot customize the width of your cells in a Zeppelin notebook. -- **False**
5. You can use multiple programming languages within the same Zeppelin notebook, and they can even operate on the same data. -- **True**

RStudio IDE is a tool for programming in R (only R).

You can upload files directly into RStudio IDE from your computer.

Generally speaking, why you should use the text editor to write code, rather than in the console? Using a text editor will provide you a larger space for you to write your code that you can save and even describe what your code does.

In R, what are datasets in tabular format typically called? Dataframes.

*Quiz*

1. RStudio IDE allows you to -- **analyze data; see your console; visualize your plots and graphs**
2. The reason why 'sc' is one of the default variables in RStudio IDE on Skills Network Labs is because -- **'sc' stands for "SparkContext" and is created by default to enable users to use Apache Spark from RStudio IDE**
3. What are ways that you can execute R code in RStudio? -- **from the script editor & console**
4. You can view a history of code you have previously executed on RStudio IDE. -- **True**
5. Which of the following options are True? -- you can use built-in datasets (dataframes) in RStudio, such as "mtcars".


## Open Source tools for Data Science -- Week 3

Jupyter Notebooks and RStudio are both available on IDM Data Science Experience.

In order to create a Project, you must first point to two resources: a storage type and a Spark service.

In Markdown, to ensure two lines of text are rendered as two separate lines of text, what should you do? Add at least two empty spaces at the end of the first line.

*Quiz*

1. What is the key difference between Skills Network Labs and IBM Watson Studio (Data Science Experience)? -- **IBM Watson Studio is an enterprise-ready environment with scalability in mind, whereas Skills network Labs is primarily a learning environment**
2. The only way to add notebooks to your project on IBM Watson Studio is to either create an empty notebook, or upload a notebook from your computer. -- **False**
3. Which of the following options are correct? -- **you can add collaborators to projects; collaborators on a project can add comments to notebooks; you can use Python 3, R or Scala with Jupyter Notebooks on Watson Studio**
4. In the upper right-hand corner of a Jupyter Notebook is the kernel interpreter (e.g. Python 3). Next to it is a circle. What does it mean if the circle is full, black circle and what should you do? -- **A black circle means that the kernel is not ready to execute more code. You can either wait, interrupt, or try to restart the kernel**
5. RStudio is available on IBM Watson Studio. -- **True**


## Data Science Methodology -- Week 1

In data mining, the Cross Industry Process for Data Mining (CRISP-DM) methodology is widely used. The CRISP-DM methodology is a process aimed at increasing the use of data mining over a wide variety of business applications industries. The intent is to take case specific scenarios and general behaviors to make them domain neutral. CRISP-DM is comprised of six steps with an entity that has to implement in order to have a reasonable chance of success.

![](images/CRISP-DM.png)

1. Business Understanding
2. Data Understanding
3. Data Preparation
4. Modelling
5. Evaluation
6. Deployment

### From Problem to Approach

Data science methodology begins with spending the time to seek clarification, to attain what can be referred to as a *business understanding*. (**1st in methodology**)

Establishing a clearly defined question starts with understanding the goal of the person asking the question.

Once the problem to be addressed is defined, the appropriate *analytic approach* for the problem is selected in the context of the business requirements. (**2nd in methodology**)

* If the question is to determine probabilities of an action -- use a predictive model
* If the question is to show relationships -- use a descriptive model
* If the question requires a yes/no answer -- use a classification model

Although the analytics approach is the second stage of the data science methodology, it is still independent of the business understanding stage. -- False (it is only when the problem to be addressed is defined, that the appropriate analytic approach for the problem can e selected in context of the business requirements)

*Quiz*

1. A methodology is an application for a computer program. -- **False**
2. The first stage of the data science methodology is Data Understanding. -- **False**
3. Business Understanding is an important stage in the data science methodology. Why? -- **Because it clearly defines the problem and the needs from a business perspective**
4. Which of the following statements about the analytic approach are correct? -- **if the question defined in the business understanding stage can be answered by determining probabilities of an action, then a predictive model would the right analytic approach; if the question defined in the business understanding deals with exploring relationships between different factors, then a descriptive approach, where clusters of similar activities based on events and preferences are examined, would be the right analytic method**
5. For the case study, a decision tree classification model was used to identify the combination of conditions leading to each patient's outcome. -- **True**

### From Requirements to Collection

Prior to undertaking the data collection and data preparation stages of the methodology, it is vital to define the *data requirements*, including identifying the necessary data content, formats and sources for initial data collection. (**3nd in methodology**)

Congestive heart failure patients with other significant medical conditions were included in the study in order to increase the sample size of the patients included in the study. -- False (congestive heart failure patients with other significant medical conditions were actually excluded from the study)

In this phase, the data requirements are revised and decisions are made as to whether or not the collection requires more or less data. Once the data ingredients are collected, then in the *data collection* stage, the data scientist will have a good understanding of what they will be working with. (**4th in methodology**)

When collecting data, it is alright to defer decisions about unavailable data, and attempt to acquire it at a later stage.

*Quiz*

1. Which of the following analogies is used in the videos to explain the Data Requirements and Data Collection stages of the data science methodology? -- **you can think of the Data Requirements and Data Collection stages as a cooking task, where the problem at hand is a recipe, and the data to answer the question is the ingredients**
2. The Data Requirements stage of the data science methodology involves identifying the necessary data content, formats and sources for initial data collection. -- **True**
3. Database Administrators determine how to collect and prepare the data. -- **False**
4. In the Data Collection stage, the business understanding of the problem is revised and decisions are made as to whether or not more data is needed. -- **False (not business understanding is revised but data requirements)**
5. Techniques such as supervised methods and unsupervised methods can be applied to the dataset, to assess the content, quality, and initial insights about the data. -- **False**


## Data Science Methodology -- Week 2

### From Understanding to Preparation

*Data Understanding* encompasses all activities related to constructing the dataset. Essentially, the *data understanding* section of the data science methodology answers the question: Is the data that you collected representative of the problem to be solved? (**5th in methodology**)

Transforming data in the *data preparation* phase is the process of getting the data into a state where it may be easier to work with. Specifically, the data preparation stage of the methodology answers the question: What are the ways in which data is prepared? (**6th in methodology**) 

To work effectively with the data, it must be prepared in a way that addresses missing or invalid values and removes duplicates, towards ensuring that everything is properly formatted.

The Data Preparation stage is the least time-consuming phase of a data science project, typically taking between 5 to 10 percent of the overall project time. -- False (most, 70 to 90 percent)

In the case study, the target variable was congestive heart failure (CHF) with 45 days following discharge from CHF hospitalization. -- False (45)

*Quiz*

1. The Data Understanding stage refers to the stage of removing redundant data. -- **False**
2. In the case study, during the Data Understanding stage, data scientists discovered that not all the congestive heart failure admissions that were expected were being captured. What action did they take to resolve the issue? -- **the data scientists looped back to the Data Collection stage, adding secondary and tertiary diagnoses, and building a more comprehensive definition of congestive heart failure admission**
3. The Data Preparation stage involves correcting invalid values and addressing outliers. -- **True**
4. Selecting the correct statement about what data scientists do during the Data Preparation stage. -- **data scientists define the variables to be used in the model; data scientists determine the timing of events; data scientists aggregate the data and merge them from different sources; data scientists identify missing data**
5. The Data Preparation stage is a very iterative and complicated stage that cannot be accelerated through automation. -- **False**

### From Modelling to Evaluation

*Data Modelling* focuses on developing models that are either descriptive or predictive. (**7th in methodology**)

1. Understand the question at hand
2. Select an analytic approach or method to solve the problem
3. Obtain, understand, prepare, and model the data

A training set is used to build a predictive model.

In the case study, the best performing model was the second model, with a relative cost of 4:1 and an overall accuracy of 85%. -- False (third one, 4:1, and 81%)

*Model Evaluation* is performed during model development and before the model is deployed. Evaluation allows the quality of the model to be assessed but it's also an opportunity to see if it meets the initial request. (**8th in methodology**)

The ROC curve is a useful diagnostic tool in determining the optimal classification model. This curve quantifies how well a binary classification model performs, declassifying the yes and no outcomes when some discrimination criterion is varied.

Model evaluation can have two main phases: a diagnostic measure phase and statistical significance testing.

*Quiz*

1. Select the correction statement. -- **a training set is used for predictive modelling**
2. A statistician calls a false-negative, a type I error, and a false-positive, a type II error. -- **False** (false-negative == type II; false-positive == type II)
3. The Modelling stage is followed by the Analytic Approach stage. -- **False**
4. Model Evaluation includes ensuring that the data are properly handled and interpreted. -- **True**
5. The ROC curve is a useful diagnostic tool for determining the optimal classification model. -- **True**


## Data Science Methodology -- Week 3

### From Deployment to Feedback

Once the model is evaluated and the data scientist is confident it will work, it is *deployed* and put to the ultimate test. Depending on the purpose of the model, it may be rolled out to a limited group of users or in a test environment, to build up confidence in applying the outcome for use across the board. (**9th in methodology**)

Once in play, *feedback* from the users will help to refine the model and assess it for performance and impact. The value of the model will be dependent on successfully incorporating *feedback* and making adjustments for as long sa the solution is required. (**10th in methodology**) 

The data science methodology is highly iterative, ensuring the refinement at each stage in the game.

Thinking like a data scientist:
1. forming a concrete business or research problem
2. collecting and analyzing data
3. building a model
4. understanding the feedback after model deployment

Learning how to work with data:
1. determining the data requirements
2. collecting the appropriate data
3. understanding the data
4. preparing the data for modelling

Learning how to derive the answer:
1. evaluating and deploying the model
2. getting feedback on it
3. using that feedback constructively so as to improve the model

*Quiz*

1. The final stages of the data science methodology are an iterative cycle between which of the different stages? -- **modelling, evaluation, deployment and feedback**
2. Feedback is not required once the model is deployed because the Model Evaluation stage would have assessed the model and made sure that it performed well. -- **False**
3. What does deploying a model into production represent? -- **it represents the beginning of an iterative process that includes feedback, model refinement and redeployment and requires the input of additional groups, such as marketing personnel and business owners**
4. The data science methodology is a specific strategy that guides processes and activities relating to data science only for text analytics. -- **False**
5. A data scientist determines that building a recommender system is the solution for a particular business problem at hand. What stage of the data science methodology does this represent? -- **analytic approach**
6. A car company asked a data scientist to determine what type of customers are more likely to purchase their vehicles. However, the data comes from several sources and is in a relatively raw format. What kind of processing can the data scientist perform on the data to prepare it for Modelling stage? -- **feature engineering; transforming the data into more useful variables; combining the data from the various sources; addressing missing invalid values**
7. What do data scientists typically use for exploratory analysis of data and to get acquainted with it? -- **they use descriptive statistics and data visualization techniques**
8. Which of the following represent the two important characteristics of the data science methodology? -- **it is a highly iterative process and it never ends**
9. For predictive models, a test set, which is similar to - but independent of - the training set, is used to determine how well the model predicts outcomes. This is an example of what step in the methodology? -- **model evaluation**
10. Data scientists should maintain continuous communication with stakeholders throughout a project so that business stakeholders can ensure the work remains on track to generate the intended solution. -- **True**

![](images/methodology.png)


## Python for Data Science and AI -- Week 1

This Python course consists of the following modules: Python basics, Python data structures, Python programming fundamentals, working with Data in Python, final project.

*Quiz*

1. What is the type of the following: 1.0? -- **float**
2. What is the type of the following: "7.1"? -- **string**
3. What is the result of the following code segment: int(12.3)? -- **12**
4. What is the result of the following code segment: int(True)? -- **1**

*Quiz*

1. What is the result of the following code segment: 1/2? -- **0.5**
2. What is the value of x after the following lines of code? x=2; x=x+2 -- **4**
3. What is the result of the following operation 3+2*2? -- **7**
4. In Python3, what is the type of the variable x after the following: x=2/2? -- **float**

*Quiz*

1. In Python, if you executed name = 'Lizz', what would be the output of print(name[0:2])? -- **Li**
2. Consider the string A = '1934567', what is the result of the following operation A[1::2]? -- **'946'**
3. In Python, what is the result of the following operation: '1'+'2'? -- **'12'**
4. Given myvar = 'hello', how would you return myvar as uppercase? -- **myvar.upper()**
5. Consider the string Name = 'Michael Jackson', what is the result of the following operation Name.find('el')? -- **5**
6. What is the result of the following: str(1)+str(1)? -- **'11'**
7. What is the result of the following: "123".replace("12", "ab")? -- **"ab3"**


## Python for Data Science and AI -- Week 2

*Quiz*

1. What is the syntax to obtain the first element of the tuple: A=('a','b','c')? -- **A[0]**
2. Consider the tuple A=((11,12),[21,22]), that contains a tuple and list. What is the result of the following operation A[1]: -- **[21,22]**
3. Consider the tuple A=((11,12),[21,22]), that contains a tuple and list. What is the result of the following operation A[0][1]: -- **12**
4. What is the result of the following operation: '1,2,3,4'.split(',') -- **['1','2','3','4']**
5. After applying the following method, L.append(['a','b']), the following list will be only one element longer. -- **True**
6. Lists are mutable. -- **True**
7. Consider the following list: A=['hard rock',10,1.2]. What will list A contain after the following command is run: del(A[0])? -- **[10,1.2]**
8. If A is a list what does the following syntax do: B=A[:]? -- **variable B references a new copy or clone of the original list A**
9. What is the result of the following: len(('disco', 10))? -- **2**

*Quiz*

1. Consider the following dictionary: {"The Bodyguard": "1992", "Saturday Night Fever": "1977"}, select the keys: -- **"The Bodyguard" & "Saturday Night Fever"**
2. The variable release_year_dict is a Python dictionary, what is the result of applying the following method: release_year_dict.values() -- **retrieves the values of the dictionary**
3. How many identical keys can a dictionary have? -- **0**

*Quiz*

1. How do you cast the list A to the set a? -- **a = set(A)**
2. Consider the Set: V={'1','2'}, what is the result of V.add('3')? -- **{'1','2','3'}**
3. What is the result of the following: '1' in {'1','2'}? -- **True**


## Python for Data Science and AI -- Week 3

*Quiz*

1. What value of x will produce the output: "Hello\nMike"? -- **x="A"**
2. What is the output of the following code? -- **"Go Mike"**
3. What is the result of the following lines of code? -- **True**

*Quiz*

1. What is the output of the following few lines of code? -- **2/4/6**
2. What is the output of the following few lines of code? -- **5/4/3**
3. What is the output of the following few lines of code? -- **1 A / 2 B / 3 C**
4. What is the output of the following? -- **2**

*Quiz*

1. Consider the function delta, when will the function return a value of 1? -- **when the input is 0**
2. Given the function add shown below, what does the following return? -- **'11'**
3. What is the correct way to sort list 'B' using a method, the result should not return a new list, just change the list 'B'? -- **B.sort()**
4. What is the output of the following lines of code? -- **2**

*Quiz*

1. Consider the class Rectangle, what are the data attributes? -- self.height, self.width, self.color
2. What is the result of running the following lines of code? -- x=A y=B
3. What is the result of running the following lines of code? -- x=A y=2


## Python for Data Science and AI -- Week 4

*Quiz*

1. Consider the following text file. What is the output of the following lines of code? -- **This is line 1**
2. Consider the file object: File1. How would you read the first line of text? -- **File1.readline()**
3. What do the following lines of code do? -- **read the file "Example1.txt"**

*Quiz*

1. Consider the following line of code. What mode is the file object in? -- **write**
2. What do the following lines of code do? -- **append to the file "Example.txt"**
3. What task do the following lines of code perform? -- **copy the text from Example2.txt to Example3.txt**

**Using loc, iloc and ix**

*loc* is primarily label based. When two arguments are used, you use column headers and row indexes to select the data you want. *loc* can also take an integer as a row or column number. *loc* will return a *KeyError* if the requested items are not found.

*iloc* is integer-based. You use column numbers and row numbers to get rows or columns at particular positions in the data frame. *iloc* will return a *KeyError* if the requested indexer is out-of-bounds.

*ix* looks for a label. If *ix* does not find a label, it will use an integer. This means you can select data by using either column numbers and row numbers or column headers and row names using *ix*. (deprecated after pd 0.20.0)

*Quiz*

1. What is the result of applying the following method df.head() to dataframe df? -- **print the first 5 rows of the dataframe**
2. Consider the dataframe df, how would you access the element in the 1st row 3rd column? -- **df.ix[0,2]**
3. In the lab, you learned you can also obtain a series from a dataframe df. Select the correct way to assign the column with header Length to a pandas series to the variable x. -- **x = df['Length']**

*Quiz*

1. What is the result of the following lines of code? -- **array([0,0,0,0,0])**
2. What is the result of the following lines of code? -- **0**
3. What is the result of the following lines of code? -- **array([11,11,11,11,11])**

*Quiz*

1. How do you perform matrix multiplication on the numpy arrays A and B? -- **np.dot(A,B)**
2. What values does the variable out take if the following lines of code are run? -- **array([0,1])**
3. What is the value of Z after the following code is run? -- **array([[2,2],[2,2]])**

An API lets two pieces of software talk to each other. Pandas, an example of API, is actually a set of software components much of which are not even written in Python.


## Python for Data Science and AI -- Week 5

Extracting essential data from a dataset and displaying it is a necessary part of data science; therefore, individuals can make correct decisions based on data.


## Databases and SQL for Data Science -- Week 1

SQL is powerful language that is used for communicating with databases. Here are some of the advantages of learning SQL for someone interested in data science:
1. SQL will boost your professional profile as a data scientist
2. Learning SQL will give you a good understanding of relational databases
3. If you work with reporting tools that generate SQL queries for you, it might be useful to write your own SQL statements 

**What is SQL**

SQL (Structured Query Language) is a language used for relational databases and for querying data.

**What is data**

Data is a collection of facts in the form of words, numbers or even pictures. Data is one of the most critical assets of any business. Data is important, so it needs to be secure and it needs to be stored and accessed quickly.

**What is a database**

A database is repository of data. It is a program that stores data. A database also provides the functionality of adding, modifying and querying that data. Different kinds of databases store data in different forms. Data stored in tabular form is a **relational database**.

Which of the following statements are correct about databases?
* A database is a repository or logically coherent collection of data with some inherent meaning.
* Typically comes with functionality for adding, modifying and querying data.
* SQL or Structured Query Language is commonly used for accessing data in relational databases.

**What is an RDBMS**

RDBMS, or Relational DataBase Management System, is a set of software tools that controls the data: access, organization and storage. 

A relational database stores data in a tabular format - i.e. in rows and columns. But not all types of databases use the tabular format.

A Cloud Database is a database service built and accessed through a cloud platform. Benefits are: ease of use / scalability / disaster recovery. Database services are logical abstractions for managing workloads in a database. An instance of the Cloud Database operates as a service that handles all applications requests to work with the data in any of the databases managed by that instance.

*Quiz*

1. Which of the following statements are correct about databases? -- **a database is repository of data; there are different types of databases - Relational, Hierarchical, No SQL, etc; a database can be populated with data and be queried**
2. Which of the following statements about a Database is/are correct? -- **a database is a logically coherent collection of data with some inherent meaning**
3. Select the correct statement below about database services or database instances: -- **database services are logical abstractions for managing workloads in a database; an instance of the Cloud database operates as a service that handles all application requests to work with the data in any of the databases managed by that instance; the database service instance is the target of the connection requests from applications**
4. The 5 basic SQL commands are: -- **CREATE, SELECT, INSERT, UPDATE, DELETE**
5. A database stores data in tabular form only. -- **False**

There are 5 simple statements:
* create table
* insert data
* select data
* update data
* delete data

DDL (Data Definition Language) statements: define / change / drop data

DML (Data Manipulation Language) statements: read / modify data

The Primary Key in a relational table prevents duplicate rows in that table. -- True (the primary key of a relational table uniquely identifies each row in a table)

```
CREATE TABLE author
    (author_id CHAR(2) CONSTRAINT AUTHOR_PK PRIMARY KEY (author_id) NOT NULL,
    lastname VARCHAR(15) NOT NULL,
    firstname VARCHAR(15) NOT NULL,
    email VARCHAR(40),
    city VARCHAR(15),
    country CHAR(2)
    );
DROP TABLE author;
```

The main purpose of a database management system is not just to store the data but also facilitate retrieval of the data. The SELECT statement is called a query and the output we get from executing this query is called a result set or a result table.

You can retrieve just the columns you want. The order of the columns displayed always matches the order in the SELECT statement. WHERE clause helps to restrict the result set, which always requires a Predicate: True, False or Unknown. (not equal to <>)

```
SELECT ID,NAME FROM COUNTRY;
SELECT * FROM COUNTRY;
SELECT * FROM COUNTRY WHERE CCODE='CA';
```

COUNT() is a built-in function that retrieves the number of rows matching the query criteria. 

```
SELECT COUNT(COUNTRY) FROM MEDALS WHERE COUNTRY='CANADA';
```

DISTINCT is used to remove duplicate values from a result set.

```
SELECT DISTINCT COUNTRY FROM MEDALS WHERE MEDALTYPE='GOLD';
```

LIMIT is used for restricting the number of rows retrieved from the database.

```
SELECT * FROM MEDALS WHERE YEAR=2018 LIMIT 5
```

INSERT statement is used to populate the table with data. A single INSERT statement can be used to insert one or multiple rows in a table.

```
INSERT INTO AUTHOR
    (AUTHOR_ID, LASTNAME, FIRSTNAME, EMAIL, CITY, COUNTRY)
VALUES
    ('A1', 'CHONG', 'RAUL', 'RFC@IBM.com', 'TORONTO', 'CA'),
    ('A2', 'ZIHENG', 'ZHANG', 'ZZH@IBM.com', 'SHENZHEN', 'CH')
```

UPDATE statement is used to alter the data when the table is created and data is inserted into the table. 

```
UPDATE AUTHOR
SET     LASTNAME = KETTA
        FIRSTNAME = LAKSHMI
WHERE   AUTHOR_ID = A2
```

DELETE statement is used to remove one or more rows from the table.

```
DELETE FROM AUTHOR
WHERE AUTHOR_ID IN ('A2','A3')
```
if no WHERE clause is used, all rows will be removed.

*Quiz*

1. The primary key of a relational table uniquely identifies each rwo in a table. -- **True**
2. The INSERT statement cannot be used to insert multiple rows in a single statement. -- **False**
3. The SELECT statement is called a Query, and the output we get from executing the query is called a Result Set. -- **True**
4. The CREATE TABLE statement is a -- **DDL statement**
5. What are the basic categories of the SQL language based on functionality? -- **Data Definition Language and Data Manipulation Language**

Information models and data models are different and serve different purposes. An information model is at the conceptual level and defines relationship between objects. Data models are defined in a more concrete level, are specific and include details. A data model is a blueprint of any database system. 

**Relational Model**

* most used data model
* allows for data independence (logical data / physical data / physical storage)
* data is stored in tables

An entity-relationship data model or ER data model is an alternative to a relational data model.

Which statement below is correct about the relational model? -- data is organized in tables with entity relationships

The ER model is used as a tool to design relational databases. In ER models, entities are objects that exist independently of any other entities in the database. Building blocks of ER models are *entities* and *attributes*. Entities have attributes, which are the data elements that characterize the entity.

![](images/model_ER.jpg)

Defining relationships between entities:
* one-to-one relationship
* one-to-many relationship
* many-to-many relationship

**ER Diagrams** are basic foundation for designing a database. Begin with ERD, and map the ERD to the table.

Which of the following statements about Entity Relationship Diagrams (ERD) is true? -- attributes in an ERD are mapped to columns in a relational table; entities in an ERD are mapped to tables in a relational table

In a relation: **degree** = the number of attributes in a relation; **cardinality** = the number of tuples. Rows in a relational instance (or a table) are also known as Tuples. If a table has 4 columns and 8 rows then its cardinality is 4.

*Quiz*

```
CAR DEALERSHIP DATABASE
CAR: serial_no, model, manufacturer, price
SALE: salesperson_id, serial_no, date, sale_price
SALESPERSON: salesperson_id, name, phone
```
1. How many relations does the Car Dealership Schema contain? -- **3**
2. How many attributes does the relation CAR contain? -- **4**
3. What is the degree of the relation Salesperson? -- **3**

*Quiz*

1. Advantages of the relational model include: -- **it is the most used data model; data is stored in simple data structures such as tables; provides logical and physical data independence**
2. A table containing one or more foreign keys is called a Dependent table. -- **True**
3. The Primary Key of a relational table uniquely identifies each __ in a table. -- **row**


## Databases and SQL for Data Science -- Week 2

Retrieving rows from a table

```
SELECT * FROM BOOK;
SELECT BOOK_ID, TITLE FROM BOOK;
SELECT BOOK_ID, TITLE FROM BOOK WHERE BOOK_ID='B1'
```

In a relational database, we can use string patterns to search data rows that match this condition. The **LIKE** predicate is used in a WHERE clause to search for a pattern in a column. We can also use the comparison operators or **BETWEEN AND** in a WHERE clause (the values in the range are inclusive). We can use **IN** operator to specify a set of values in a WHERE clause.

```
SELECT FIRSTNAME FROM AUTHOR WHERE FIRSTNAME LIKE 'R%';
SELECT TITLE, PAGES FROM BOOK WHERE PAGE >= 290 AND PAGES <= 300;
SELECT TITLE, PAGES FROM BOOK WHERE PAGES BETWEEN 290 AND 300;
SELECT FIRSTNAME, LASTNAME, COUNTRY FROM AUTHOR WHERE COUNTRY='AU' OR COUNTRY='BR';
SELECT FIRSTNAME, LASTNAME, COUNTRY FROM AUTHOR WHERE COUNTRY IN ('AU', 'BR')
```

To display the result set in alphabetical order, we add the **ORDER BY** clause to the SELECT statement (ascending by default; descending using **DESC**). In addition, we can specify column sequence number in ORDER BY clause.

```
SELECT TITLE FROM BOOK ORDER BY TITLE;
SELECT TITLE FROM BOOK ORDER BY TITLE DESC;
SELECT TITLE, PAGES FROM BOOK ORDER BY 2; (order by pages)
```

To eliminate duplicates in the result set, we use the keyword **DISTINCT**. Besides, the **GROUP BY** clause groups a result set into subsets that has matching values for one or more columns. We can further restrict the number of rows with **HAVING** clause in the GROUP BY clause.

```
SELECT DISTINCT(COUNTRY) FROM AUTHOR;
SELECT COUNTRY, COUNT(COUNTRY) FROM AUTHOR GROUP BY COUNTRY;
SELECT COUNTRY, COUNT(COUNTRY) AS COUNT FROM AUTHOR GROUP BY COUNTRY;
SELECT COUNTRY, COUNT(COUNTRY) AS COUNT FROM AUTHOR GROUP BY COUNTRY HAVING COUNT(COUNTRY) > 4
```

*Quiz*

1. You want to select author's last name from a table, but you only remember the author's last name starts with the letter B, which string pattern can you use? -- ```SELECT FIRSTNAME FROM AUTHOR WHERE LASTNAME LIKE 'B%'```
2. In a SELECT statement, which SQL clause controls how the result set is displayed? -- **ORDER BY clause**
3. Which SELECT statement eliminates duplicates in the result set? -- ```SELECT DISTINCT(COUNTRY) FROM AUTHOR```
4. What is the default sorting mode of the ORDER BY clause? -- **ascending**
5. Which of the following can be used in a SELECT statement to restrict a result set? -- **HAVING / WHERE / DISTINCT**

Most databases come with built-in SQL functions. Built-in functions can be included as part of SQL statements. Database functions significantly reduce the amount of data that needs to be retrieved. Built-in functions can speed up data processing.

**Aggregate or Column Functions**

* INPUT: collection of values (e.g. entire column)
* OUTPUT: single value
* e.g. SUM(), MIN(), MAX(), AVG() etc

```
SELECT SUM(SALESPERSON) FROM PETSALE;
SELECT SUM(SALESPERSON) AS SUM_OF_SALESPERSON FROM PETSALE;
SELECT MAX(QUANTITY) FROM PETSALE;
SELECT MIN(ID) FROM PETSALE WHERE ANIMAL='DOG';
SELECT AVG(SALEPRICE) FROM PETSALE;
SELECT AVG(SALEPRICE / QUANTITY) FROM PETSALE WHERE ANIMAL='DOG'
```

**Scaler and String Functions**

performs operations on every input values. e.g. ROUND(), LENGTH(), UCASE(), LCASE() etc

```
SELECT ROUND(SALEPRICE) FROM PETSALE;   round up/down every value
SELECT LENGTH(ANIMAL) FROM PETSALE;     retrieve length of each value
SELECT UCASE(ANIMAL) FROM PETSALE;
SELECT * FROM PETSALE WHERE LCASE(ANIMAL)='cat';
SELECT DISTINCT(UCASE(ANIMAL)) FROM PETSALE
```

**Date and Time Functions**

Most databases contain special data types for dates and times.
* DATE ```YYYYMMDD```
* TIME ```HHMMSS```
* TIMESTAMP ```YYYYMMDDHHMMSSZZZZZZ```

Date / Time functions: ```YEAR(), MONTH(), DAY(), DAYOFMONTH(), DAYOFWEEK(), DAYOFYEAR(), WEEK(), HOUR(), MINUTE(), SECOND()```

Special registers: ```CURRENT_DATE, CURRENT_TIME```

```
SELECT DAY(SALEDATE) FROM PETSALE WHERE ANIMAL='CAT';
SELECT COUNT(*) FROM PETSALE WHERE MONTH(SALEDATE)='05';
SELECT (SALEDATE + 3 DAYS) FROM PETSALE;
SELECT (CURRENT_DATE - SALEDATE) FROM PETSALE)
```

**Sub-query** is a query inside another query. This allows you to form more powerful queries than would have been otherwise possible.

```
SELECT COLUMN1 FROM TABLE WHERE COLUMN2 = (SELECT MAX(COLUMN2) FROM TABLE);
SELECT EMP_ID, F_NAME, L_NAME, SALARY FROM EMPLOYEES WHERE SALARY < (SELECT AVG(SALARY) FROM EMPLOYEES);
SELECT EMP_ID, SALARY, (SELECT AVG(SALARY) FROM EMPLOYEES) AS AVG_SALARY FROM EMPLOYEES;
SELECT * FROM (SELECT EMP_ID, F_NAME, L_NAME, DEP_ID FROM EMPLOYEES) AS EMP4ALL
```

There are several ways to access multiple tables in the same query:
1. sub-queries
2. implicit JOIN
3. JOIN operators (INNER JOIN, OUTER JOIN, etc)

```
SELECT * FROM EMPLOYEES WHERE DEP_ID IN (SELECT DEPT_ID_DEP FROM DEPARTMENTS);
SELECT * FROM EMPLOYEES WHERE DEP_ID IN (SELECT DEPT_ID_DEP FROM DEPARTMENTS WHERE LOC_ID='L0002');
SELECT DEPT_ID_DEP, DEP_NAME FROM DEPARTMENTS WHERE DEPT_ID_DEP IN (SELECT DEP_ID FROM EMPLOYEES WHERE SALARY > 70000);
SELECT * FROM EMPLOYEES, DEPARTMENTS; (full join / Cartesian join / every row in 1st is joined with every row in 2nd)
SELECT * FORM EMPLOYEES, DEPARTMENTS WHERE EMPLOYEES.DEP_ID=DEPARTMENTS.DEPT_ID_DEP;
SELECT * FROM EMPLOYEES E, DEPARTMENTS D WHERE E.DEP_ID=D.DEPT_ID_DEP;
SELECT EMPLOYEES.EMP_ID, DEPARTMENTS.DEPT_NAME FROM EMPLOYEES E, DEPARTMENTS D WHERE E.DEP_ID=D.DEPT_ID_DEP;
SELECT E.EMP_ID, D.DEP_ID_DEP FROM EMPLOYEES E, DEPARTMENTS D WHERE E.DEP_ID=D.DEPT_ID_DEP
```

*Quiz*

1. Which of the following will retrieve the LOWEST value of SALARY in a table called EMPLOYEES? -- ```SELECT MIN(SALARY) FROM EMPLOYEES```
2. Assume there exists an INSTRUCTOR table with several columns including FIRSTNAME, LASTNAME, etc. Which of the following is the most likely result set for the following query? SELECT DISTINCT(FIRSTNAME) FROM INSTRUCTOR -- **LEON / PAUL / JOE**
3. Which of the following queries will return the first name of the employee who earns the highest salary? -- ```SELECT FIRST_NAME FROM EMPLOYEES WHERE SALARY=(SELECT MAX(SALARY) FROM EMPLOYEES)```
4. Which of the following queries will return the data for employees who belong to the department with the highest value of department ID? -- ```SELECT * FROM EMPLOYEES WHERE DEP_ID=(SELECT MAX(DEPT_ID_DEP) FROM DEPARTMENTS)```
5. A DEPARTMENTS table contains DEP_NAME, and DEPT_ID_DEP columns and an EMPLOYEES table contains columns called F_NAME and DEP_ID. We want to retrieve the Department Name for each Employee. Which of the following queries will correctly accomplish this? -- ```SELECT D.DEP_NAME, E.F_NAME FROM DEPARTMENTS D, EMPLOYEES E WHERE D.DEPT_ID_DEP=E.DEP_ID```

A **Foreign Key** is a set of columns referring to a primary key of another table. It is the **Primary Key** which uniquely identifies a row in a table.
* Parent Table: a table containing a Primary Key that is related to at least one Foreign Key
* Dependent Table: a table containing one or more Foreign Keys

The following six constraints are defined in a relational database model:
1. entity integrity constraint
2. referential integrity constraint
3. semantic integrity constraint
4. domain constraint
5. null constraint
6. check constraint

If a Primary Key was allowed to have NULL values the Entity Integrity Constraint of a table could be violated. The Referential Integrity ensures the validity of the data using a combination of Primary Keys and Foreign Keys.

*Quiz*

1. Which Relational Constraint prevents duplicate rows in a table? -- **Entity Integrity Constraint**
2. Which Relational Constraint ensures the validity of the data using a combination of Primary Keys and Foreign Keys? -- **Referential Integrity Constraint**
3. Which of the following statements are correct about the primary keys? -- **the value of the Primary Key must be unique for each instance of the entity; the Primary Key is immutable, i.e., once created the value of the Primary Key cannot be changed; Primary Keys cannot have missing or NULL values**
4. Which of the following statement is true? -- **a Foreign Key refers to a Primary Key of another table**


## Databases and SQL for Data Science -- Week 3

There are many benefits of Python for database programming:
* ecosystem: NumPy, Pandas, matplotlib, SciPy
* ease of use
* portable
* database APIs
* support for relational database systems
* detailed documentation

Notebooks are very popular in the field of data science because they run in an environment that allows creation and sharing of documents that contain live code, equations, visualizations and explanatory texts. A notebook interface is a virtual notebook environment used for programming.

SQL API consists of a library of function calls as an application programming interface (API) for the DBMS.

![](images/SQL-API.png)

Two concepts of the Python DB API:
* connection objects
  * database connections
  * manage transaction
* cursor objects
  * database queries

Cursors created from the same connection are not isolated, i.e. any changes done to the database by a cursor are immediately visible to the other cursors. Cursors created from different connections can or cannot be isolated depending on how the transaction support is implemented.

A database cursor is a control structure that enables traversal over the records in a database. It behaves like a file name or file handle in a programming language.

```
from dbmodule import connect

# create connection object
Connection = connection('databasename', 'username', 'pswd')

# create a cursor object
Cursor = Connection.cursor()

# run queries
Cursor.execute('SELECT * FROM MYTABLE')
Results = Cursor.fetchall()

# free resources
Cursor.close()
Connection.close()
```

The ```ibm_db``` API provides a variety of useful Python functions for accessing and manipulating data in an IBM data server Database.

The ```ibm_db.exec_immediate()``` function prepares and executes a SQL statement. The parameters passed to the ```ibm_db.exec_immediate``` function include: connection; statement; options.

*Quiz*

1. A database cursor is a control structure that enables traversal over the records in a database. -- **True**
2. The ```ibm_db``` API provides a variety of useful Python functions for accessing and manipulating data in an IBM data server like Db2. -- **True**
3. A DataFrame represents a tabular, spreadsheet-like data structure containing an ordered collection of columns, each of which can be a different value type. A pandas dataframe in Python can be used for storing the result set of a SQL query. -- **True**
4. Which of the following statement about Python is NOT correct? -- **due to its proprietary nature, database access from Python is not available for many databases**
5. To query data from tables in database a connection to the database needs to be established. Which of the following is NOT required to establish a connection with a relational database from a Python network. -- **table and column names**

A simple SELECT statement retrieves data from one or more columns from a single table. The next level of complexity is retrieving data from two or more tables. To combine data from two tables, we use **JOIN** operator: 1) combines row from two or more tables; 2) based on a relationship.

* PrimaryKey-ForeignKey is the common JOIN operator
* JOIN operator is used to combine more than one table
* You have to know the relationship between the tables

**INNER JOIN**
> most popular

An INNER JOIN operation returns only data from the rows in the tables that match the inner join criteria.

```
SELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE 
    FROM BORROWER B INNER JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID
SELECT B.LASTNAME, L.COPY_ID, C.STATUS
    FROM BORROWER B INNER JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID
                    INNER JOIN COPY C ON L.COPY_ID=C.COPY_ID
```

**LEFT OUTER JOIN**

A LEFT (OUTER) JOIN operation matches the results from two tables and displays *all the rows from the left table*, and combines the information with rows from the right table that matches the criteria specified in the query.

```
SELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE
    FROM BORROWER B LEFT JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID
```

When using a LEFT JOIN, if the right table does not have a corresponding value, a NULL value was returned.

**RIGHT OUTER JOIN**

A RIGHT (OUTER) JOIN operation matches the results from two tables and displays *all the rows from the right table*, and combines the information with rows from the left table that matches the criteria specified in the query.

```
SELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE
    FROM BORROWER B RIGHT JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID
```

If you do not specify what type of OUTER JOIN you want to perform, it defaults to RIGHT OUTER JOIN. -- False (you need to specify which type of OUTER JOIN you want to perform - LEFT, RIGHT OR FULL)

**FULL OUTER JOIN**

The FULL (OUTER) JOIN operation returns all rows from both tables, all rows from the left table and all rows from the right table. So, the FULL JOIN could return a very large result set.

```
SELECT B.BORROWER_ID, B.LASTNAME, B.COUNTRY, L.BORROWER_ID, L.LOAN_DATE
    FROM BORROWER B FULL JOIN LOAN L ON B.BORROWER_ID=L.BORROWER_ID
```

*Quiz*

1. An INNER JOIN returns only the rows that match. -- **True**
2. A LEFT OUTER JOIN displays all the rows from the right table, and combines matching rows from the left table. -- **False**
3. When using an OUTER JOIN, you must explicitly state what kind of OUTER JOIN you want - a LEFT JOIN, a RIGHT JOIN, or a FULL JOIN. -- **True**
4. Which of the following are valid types of JOINs? -- **LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN**
5. A FULL JOIN returns only the rows that match. -- **False**


## Databases and SQL for Data Science -- Week 4

Many of the real-world datasets are made available as .csv files. These are text files which contain data values typically separated by commas.

When querying column names with mixed (upper and lower) case, we can use double quotes to specify mixed-case column names. By default, spaces are mapped to underscores. Other characters may also get mapped to underscores.

```
SELECT "Id", "Name_of_Dog", "Breed__dominant_breed_if_not_pure_breed_" FROM DOGS
```

We can use backslash \ as the escape character in cases where the query contains single quotes.

```
selectQuery = 'SELECT * FROM DOGS WHERE "Name_of_Dog"=\'Huggy\''
```

We can use backslash \ to split the query into multiple lines.

```
%sql SELECT "Id", "Name_of_Dog", \
    FROM DOGS \
    WHERE "Name_of_Dog"='Huggy'
```

Or we can use ```%%sql``` in the first row of the cell in the notebook.

```
%%sql
SELECT "Id", "Name_of_Dog",
    FROM DOGS
    WHERE "Name_of_Dog"='Huggy'
```
